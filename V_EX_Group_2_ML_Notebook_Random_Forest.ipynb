{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAKOMIKOLAKO/NASA-SEES-V-EX-WG2/blob/main/V_EX_Group_2_ML_Notebook_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest Model Training**"
      ],
      "metadata": {
        "id": "d4zQCBq7ldIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kGuY9QnnJEh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading in all the modules we'll need for the project - both to train the model and display the data / results for conclusion and evaluation.\n",
        "\n",
        "References: https://colab.research.google.com/github/NASAARSET/ARSET_ML_Fundamentals/blob/main/session2/2_MODIS_Train_Eval_Session2.ipynb\n",
        "\n",
        "https://colab.research.google.com/github/NASAARSET/ARSET_ML_Fundamentals/blob/main/session2/2_MODIS_Train_Eval_Session2.ipynb\n",
        "\n",
        "Found on the [NASA ARSET Machine Learning Fundamentals Github](https://github.com/NASAARSET/ARSET_ML_Fundamentals)"
      ],
      "metadata": {
        "id": "QLvruDPcnodh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZzh1TlHnXJG"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install scikit-optimize\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio.v3 as iio\n",
        "import skimage as ski\n",
        "import os\n",
        "\n",
        "\n",
        "# Modelling\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay, mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split, KFold, cross_val_score\n",
        "from scipy.stats import randint\n",
        "from numpy import loadtxt, sqrt, mean\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.impute import SimpleImputer\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Tree Visualisation\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "import graphviz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading in our data\n"
      ],
      "metadata": {
        "id": "XudU72ecn8v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = os.getcwd()\n",
        "print(\"Current working directory:\", current_directory)\n",
        "\n",
        "model1_datasetname = 'trainingdata_latlongonly.csv' # Latitude and longitude (coordinates) only\n",
        "model2_datasetname = 'training1.csv' #  Coordinates and GLOBE data\n",
        "model3_datasetname = 'training5.csv' # Coordinates, GLOBE data, and CEO data\n",
        "model4_datasetname = 'training6.csv' # Coordinates and CEO data only (for testing)\n",
        "\n",
        "\n",
        "#dataset = loadtxt('dataset_name, delimiter=\",\", skiprows=1)  # Assume header\n",
        "#lat_long_only_dataset = loadtxt(latlongonly_dataset_name, delimiter=\",\", skiprows=1)  # Assume header\n",
        "\n",
        "# Load the dataset using pandas which handles missing values and mixed data types better\n",
        "model4_dataset = pd.read_csv(model4_datasetname, delimiter=\",\").values  # Assumes header is in the first row by default\n",
        "model3_dataset = pd.read_csv(model3_datasetname, delimiter=\",\").values  # Assumes header is in the first row by default\n",
        "model2_dataset = pd.read_csv(model2_datasetname, delimiter=\",\").values  # Assumes header is in the first row by default\n",
        "model1_dataset = pd.read_csv(model1_datasetname, delimiter=\",\").values  # Assumes header is in the first row by default\n"
      ],
      "metadata": {
        "id": "s0wHAM4b7RNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the data"
      ],
      "metadata": {
        "id": "_lIub8oMk46G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pandas = pd.read_csv(model3_datasetname)\n",
        "df_pandas2 = pd.read_csv(model1_datasetname)\n",
        "\n",
        "df_pandas.info()\n",
        "df_pandas2.info()\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'dataset' is a pandas DataFrame\n",
        "# Replace 'dataset' with the actual name of your DataFrame variable\n",
        "# sns.pairplot(df_pandas[['red_channel_peak', 'green_channel_peak', 'blue_channel_peak', 'LST']])\n",
        "#plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "#correlation_matrix = df_pandas[['red_channel_peak', 'green_channel_peak', 'blue_channel_peak', 'LST']].corr()\n",
        "#print(correlation_matrix)"
      ],
      "metadata": {
        "id": "IwHWykO22Qxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KEFlIxKr9nrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining our model and starting training (Model 3)"
      ],
      "metadata": {
        "id": "r4F-est4ovLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## VEGETATION DATA\n",
        "\n",
        "# Setup imputer to replace NaN values with the mean of each column\n",
        "# (added this for the dataset that included CEO points, as many of the values were blank)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_pandas.iloc[:, 1:26] = imputer.fit_transform(df_pandas.iloc[:, 1:26])  # Assuming your features are from column 1 to 10\n",
        "\n",
        "dataset = df_pandas.values\n",
        "\n",
        "# Define features and target\n",
        "X = dataset[:, 1:24] #skipping column 0 because it's just image id - change these values if using a dataset with a different # of columns\n",
        "y = dataset[:, 24]\n",
        "\n",
        "# Split data\n",
        "seed = 42\n",
        "test_size = 0.1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Model\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)  # Calculating R-squared\n",
        "\n",
        "print(\"Mean Squared Error: %.2f\" % mse)\n",
        "print(\"Root Mean Squared Error: %.2f\" % rmse)\n",
        "print(\"R-squared: %.2f\" % r2)\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = model.feature_importances_\n",
        "features =[\n",
        "                    'Latitude', 'Longitude', 'Impervious Surface', 'Grass', 'Dead Vegetation', 'Bare Ground',\n",
        "                    'Cultivated Vegetation', 'Bush/Scrub', 'Water (river)', 'Water (lake)', 'Confidence', 'Total securewatch dates',\n",
        "                    'Water (irrigation ditch)', 'Grass', 'Water (rivers/stream)',\n",
        "                    'Impervious Surface (no building)', 'Wetlands', 'Water (lake/ponded/container)',\n",
        "                    'Cultivated Vegetation', 'Bare Ground', 'Building', 'Trees -Canopy Cover',\n",
        "                    'Unknown', 'Bush/Scrub', 'Shadow'\n",
        "                ],\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(features, feature_importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Actual vs Predicted\n",
        "for actual, predicted in zip(y_test, y_pred):\n",
        "    print(f\"Actual: {actual}, Predicted: {predicted:.2f}\")"
      ],
      "metadata": {
        "id": "Q1T80y4P7SS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## VEGETATION DATA - K-FOLD CROSS VALIDATION\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize measures\n",
        "scores_r2 = []\n",
        "scores_mse = []\n",
        "scores_rmse = []\n",
        "scores_mae = []\n",
        "feature_importances = []\n",
        "\n",
        "\n",
        "# K-Fold Cross-validation\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluation\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Storing results\n",
        "    scores_r2.append(r2)\n",
        "    scores_mse.append(mse)\n",
        "    scores_rmse.append(rmse)\n",
        "    scores_mae.append(mae)\n",
        "\n",
        "    feature_importances.append(model.feature_importances_)\n",
        "\n",
        "mean_feature_importances = mean(feature_importances, axis=0)\n",
        "\n",
        "# Print average scores\n",
        "print(\"Average R-squared: %.2f\" % np.mean(scores_r2))\n",
        "print(\"Average Mean Squared Error: %.2f\" % np.mean(scores_mse))\n",
        "print(\"Average Root Mean Squared Error: %.2f\" % np.mean(scores_rmse))\n",
        "print(\"Average Mean Absolute Error: %.2f\" % np.mean(scores_mae))\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for idx, imp in enumerate(mean_feature_importances):\n",
        "    print(f\"Feature {idx + 1}: {imp:.4f}\")"
      ],
      "metadata": {
        "id": "VunnDC1q3vgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization"
      ],
      "metadata": {
        "id": "PKSfauWnHqfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bayesian Optimization\n",
        "\n",
        "param_space = {\n",
        "    'n_estimators': (10, 1000),  # Number of trees in the forest\n",
        "    'max_depth': (3, 50),  # Maximum depth of each tree\n",
        "    'min_samples_split': (2, 10),  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': (1, 10),  # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ('sqrt', 'log2')  # The number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    search_spaces=param_space,\n",
        "    n_iter=100,\n",
        "    cv=10,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bayes_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters found: {bayes_search.best_params_}\")"
      ],
      "metadata": {
        "id": "q_iL8umKWtwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing again based on these best params:\n",
        "\n",
        "# Best parameters found by BayesSearchCV\n",
        "best_params = bayes_search.best_params_\n",
        "\n",
        "# Initialize RandomForestRegressor with the best parameters\n",
        "\n",
        "optimized_model = RandomForestRegressor(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    min_samples_split=best_params['min_samples_split'],\n",
        "    min_samples_leaf=best_params['min_samples_leaf'],\n",
        "    max_features=best_params['max_features'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Setup K-Fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Calculate RMSE for each cross-validated fold using 'neg_root_mean_squared_error' which returns negative values\n",
        "rmse_scores = cross_val_score(optimized_model, X, y, scoring='neg_root_mean_squared_error', cv=kf, n_jobs=-1)\n",
        "\n",
        "# Convert scores to positive\n",
        "rmse_scores = -rmse_scores\n",
        "\n",
        "# Calculate average RMSE\n",
        "average_rmse = np.mean(rmse_scores)\n",
        "\n",
        "# Calculate R-squared for each fold\n",
        "r2_scores = cross_val_score(optimized_model, X, y, scoring='r2', cv=kf, n_jobs=-1)\n",
        "\n",
        "# Calculate average R-squared\n",
        "average_r2 = np.mean(r2_scores)\n",
        "\n",
        "# Calculate MAE for each fold using 'neg_mean_absolute_error' which returns negative values\n",
        "mae_scores = cross_val_score(optimized_model, X, y, scoring='neg_mean_absolute_error', cv=kf, n_jobs=-1)\n",
        "\n",
        "# Convert scores to positive\n",
        "mae_scores = -mae_scores\n",
        "\n",
        "# Calculate average MAE#\n",
        "average_mae = np.mean(mae_scores)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Average Root Mean Squared Error from K-Fold CV: {average_rmse:.2f}\")\n",
        "print(f\"Average R-squared from K-Fold CV: {average_r2:.2f}\")\n",
        "print(f\"Average Mean Absolute Error from K-Fold CV: {average_mae:.2f}\")"
      ],
      "metadata": {
        "id": "V9Cq24ntiVlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Vegetation Incorporated Model"
      ],
      "metadata": {
        "id": "X5vei20JXeiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree = model.estimators_[0]\n",
        "export_graphviz(tree, out_file='tree.dot', feature_names=[\n",
        "                    'Latitude', 'Longitude', 'Impervious Surface', 'Grass', 'Dead Vegetation', 'Bare Ground',\n",
        "                    'Cultivated Vegetation', 'Bush/Scrub', 'Water (river)', 'Water (lake)', 'Confidence', 'Total securewatch dates',\n",
        "                    'Water (irrigation ditch)', 'Grass', 'Water (rivers/stream)',\n",
        "                    'Impervious Surface (no building)', 'Wetlands', 'Water (lake/ponded/container)',\n",
        "                    'Cultivated Vegetation', 'Bare Ground', 'Building', 'Trees -Canopy Cover',\n",
        "                    'Unknown', 'Bush/Scrub', 'Shadow'\n",
        "                ],\n",
        "                filled = True, rounded = True, special_characters = True, precision = 1)\n",
        "\n",
        "with open(\"tree.dot\") as z:\n",
        "  dot_graph = z.read()\n",
        "graph = graphviz.Source(dot_graph) # Graphviz exports .dot file automatically so have to convert to png\n",
        "graph.render(\"tree\", format=\"png\") # Have to convert to png to be able to visualize\n",
        "\n",
        "# Display image\n",
        "Image(filename='tree.png') # Stop here if you want to visualize the one full decision tree (going to be really large though)\n",
        "\n",
        "# Visualizing Small/Reduced Tree and Depth Size\n",
        "model_small = RandomForestRegressor(n_estimators=10, max_depth = 3) # Limit depth to 3 levels\n",
        "model_small.fit(X_train, y_train)\n",
        "\n",
        "# Extract the small tree\n",
        "tree_small = model_small.estimators_[0]\n",
        "\n",
        "export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names=[\n",
        "                    'Latitude', 'Longitude', 'Impervious Surface', 'Grass', 'Dead Vegetation', 'Bare Ground',\n",
        "                    'Cultivated Vegetation', 'Bush/Scrub', 'Water (river)', 'Water (lake)', 'Confidence', 'Total securewatch dates',\n",
        "                    'Water (irrigation ditch)', 'Grass', 'Water (rivers/stream)',\n",
        "                    'Impervious Surface (no building)', 'Wetlands', 'Water (lake/ponded/container)',\n",
        "                    'Cultivated Vegetation', 'Bare Ground', 'Building', 'Trees -Canopy Cover',\n",
        "                    'Unknown', 'Bush/Scrub', 'Shadow'\n",
        "                ],\n",
        "                filled = True, rounded = True, special_characters = True, precision = 1)\n",
        "\n",
        "with open(\"small_tree.dot\") as k:\n",
        "  dot_graph2 = k.read()\n",
        "\n",
        "graph = graphviz.Source(dot_graph2)\n",
        "graph.render(\"small_tree\", format=\"png\")\n",
        "\n",
        "# Display image\n",
        "Image(filename='small_tree.png')"
      ],
      "metadata": {
        "id": "W1PSHZxOXkXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_calibration_data(predictions, actuals, n_bins=10):\n",
        "    bins = np.histogram_bin_edges(predictions, bins=n_bins)\n",
        "    bin_indices = np.digitize(predictions, bins) - 1  # subtract 1 to make indices zero-based\n",
        "\n",
        "    bin_means_pred = []\n",
        "    bin_means_actual = []\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        bin_mask = (bin_indices == i)\n",
        "        if np.any(bin_mask):\n",
        "            bin_means_pred.append(predictions[bin_mask].mean())\n",
        "            bin_means_actual.append(actuals[bin_mask].mean())\n",
        "\n",
        "    return np.array(bin_means_pred), np.array(bin_means_actual)\n",
        "\n",
        "# Calculate calibration data\n",
        "bin_means_pred, bin_means_actual = calculate_calibration_data(model_predict, y_test)\n",
        "\n",
        "# Plot calibration curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(bin_means_pred, bin_means_actual, marker='o', label='XGBoost')\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', label='Perfect Calibration')\n",
        "plt.xlabel('Mean Predicted Value')\n",
        "plt.ylabel('Mean Actual Value')\n",
        "plt.title('Calibration Plot')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bo7cACapFWwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latitude/Longitude Only (Baseline) Model"
      ],
      "metadata": {
        "id": "IHHhUWtrFhJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## LAT LONG ONLY DATA\n",
        "\n",
        "# Define features and target\n",
        "X = lat_long_only_dataset[:, 1:3] #skipping column 0 because it's just image id\n",
        "y = lat_long_only_dataset[:, 3]\n",
        "\n",
        "# Split data\n",
        "seed = 42\n",
        "test_size = 0.1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Model\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)  # Calculating R-squared\n",
        "\n",
        "print(\"Mean Squared Error: %.2f\" % mse)\n",
        "print(\"Root Mean Squared Error: %.2f\" % rmse)\n",
        "print(\"R-squared: %.2f\" % r2)\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = model.feature_importances_\n",
        "features = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6']  # Adjust these as per your dataset\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(features, feature_importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Actual vs Predicted\n",
        "for actual, predicted in zip(y_test, y_pred):\n",
        "    print(f\"Actual: {actual}, Predicted: {predicted:.2f}\")\n"
      ],
      "metadata": {
        "id": "eHEqtiVYFf4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using K-fold cross validation to evaluate the model's performance"
      ],
      "metadata": {
        "id": "zzVNy4WI-Ivb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## LAT LONG ONLY DATA\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize measures\n",
        "scores_r2 = []\n",
        "scores_mse = []\n",
        "scores_rmse = []\n",
        "scores_mae = []\n",
        "feature_importances = []\n",
        "\n",
        "\n",
        "# K-Fold Cross-validation\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluation\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Storing results\n",
        "    scores_r2.append(r2)\n",
        "    scores_mse.append(mse)\n",
        "    scores_rmse.append(rmse)\n",
        "    scores_mae.append(mae)\n",
        "\n",
        "    feature_importances.append(model.feature_importances_)\n",
        "\n",
        "mean_feature_importances = mean(feature_importances, axis=0)\n",
        "\n",
        "# Print average scores\n",
        "print(\"Average R-squared: %.2f\" % np.mean(scores_r2))\n",
        "print(\"Average Mean Squared Error: %.2f\" % np.mean(scores_mse))\n",
        "print(\"Average Root Mean Squared Error: %.2f\" % np.mean(scores_rmse))\n",
        "print(\"Average Mean Absolute Error: %.2f\" % np.mean(scores_mae))\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for idx, imp in enumerate(mean_feature_importances):\n",
        "    print(f\"Feature {idx + 1}: {imp:.4f}\")"
      ],
      "metadata": {
        "id": "MCuLF-mwrwSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization"
      ],
      "metadata": {
        "id": "fnhcKNZ9GXcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bayesian Optimization\n",
        "\n",
        "param_space = {\n",
        "    'n_estimators': (10, 1000),  # Number of trees in the forest\n",
        "    'max_depth': (3, 50),  # Maximum depth of each tree\n",
        "    'min_samples_split': (2, 10),  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': (1, 10),  # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ('sqrt', 'log2')  # The number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    search_spaces=param_space,\n",
        "    n_iter=100,\n",
        "    cv=10,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bayes_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters found: {bayes_search.best_params_}\")"
      ],
      "metadata": {
        "id": "sJHdksANgG3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing again based on these best params:\n",
        "\n",
        "# Best parameters found by BayesSearchCV\n",
        "best_params = bayes_search.best_params_\n",
        "\n",
        "# Initialize RandomForestRegressor with the best parameters\n",
        "\n",
        "optimized_model = RandomForestRegressor(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    min_samples_split=best_params['min_samples_split'],\n",
        "    min_samples_leaf=best_params['min_samples_leaf'],\n",
        "    max_features=best_params['max_features'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Setup K-Fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Calculate RMSE for each cross-validated fold using 'neg_root_mean_squared_error' which returns negative values\n",
        "rmse_scores = cross_val_score(optimized_model, X, y, scoring='neg_root_mean_squared_error', cv=kf, n_jobs=-1)\n",
        "\n",
        "# Convert scores to positive\n",
        "rmse_scores = -rmse_scores\n",
        "\n",
        "# Calculate average RMSE\n",
        "average_rmse = np.mean(rmse_scores)\n",
        "\n",
        "# Calculate R-squared for each fold\n",
        "r2_scores = cross_val_score(optimized_model, X, y, scoring='r2', cv=kf, n_jobs=-1)\n",
        "\n",
        "# Calculate average R-squared\n",
        "average_r2 = np.mean(r2_scores)\n",
        "\n",
        "# Calculate MAE for each fold using 'neg_mean_absolute_error' which returns negative values\n",
        "mae_scores = cross_val_score(optimized_model, X, y, scoring='neg_mean_absolute_error', cv=kf, n_jobs=-1)\n",
        "\n",
        "# Convert scores to positive\n",
        "mae_scores = -mae_scores\n",
        "\n",
        "# Calculate average MAE#\n",
        "average_mae = np.mean(mae_scores)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Average Root Mean Squared Error from K-Fold CV: {average_rmse:.2f}\")\n",
        "print(f\"Average R-squared from K-Fold CV: {average_r2:.2f}\")\n",
        "print(f\"Average Mean Absolute Error from K-Fold CV: {average_mae:.2f}\")"
      ],
      "metadata": {
        "id": "ZVRJo8jaHoLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Lat Long Only Model"
      ],
      "metadata": {
        "id": "TN6iFrZSkBTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree = model.estimators_[0]\n",
        "export_graphviz(tree, out_file='tree.dot', feature_names=['Latitude', 'Longitude'], # If I add more features, it gives me an error\n",
        "                filled = True, rounded = True,\n",
        "                special_characters = True, precision = 1)\n",
        "\n",
        "with open(\"tree.dot\") as z:\n",
        "  dot_graph = z.read()\n",
        "graph = graphviz.Source(dot_graph) # Graphviz exports .dot file automatically so have to convert to png\n",
        "graph.render(\"tree\", format=\"png\") # Have to convert to png to be able to visualize\n",
        "\n",
        "# Display image\n",
        "Image(filename='tree.png') # Stop here if you want to visualize the one full decision tree (going to be really large though)\n",
        "\n",
        "# Visualizing Small/Reduced Tree and Depth Size\n",
        "model_small = RandomForestRegressor(n_estimators=10, max_depth = 3) # Limit depth to 3 levels\n",
        "model_small.fit(X_train, y_train)\n",
        "\n",
        "# Extract the small tree\n",
        "tree_small = model_small.estimators_[0]\n",
        "\n",
        "export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = ['Latitude', 'Longitude'],\n",
        "                filled = True, rounded = True, special_characters = True, precision = 1)\n",
        "\n",
        "with open(\"small_tree.dot\") as k:\n",
        "  dot_graph2 = k.read()\n",
        "graph = graphviz.Source(dot_graph2)\n",
        "graph.render(\"small_tree\", format=\"png\")\n",
        "\n",
        "# Display image\n",
        "Image(filename='small_tree.png')"
      ],
      "metadata": {
        "id": "ncS4ifWnkW5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 (coordinates and GLOBE data only)"
      ],
      "metadata": {
        "id": "15cfG8VjOgyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## MODEL 2\n",
        "\n",
        "# Define features and target\n",
        "X = model2_dataset[:, 1:11] #skipping column 0 because it's just image id\n",
        "y = model2_dataset[:, 11]\n",
        "\n",
        "# Split data\n",
        "seed = 42\n",
        "test_size = 0.1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Model\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)  # Calculating R-squared\n",
        "\n",
        "print(\"Mean Squared Error: %.2f\" % mse)\n",
        "print(\"Root Mean Squared Error: %.2f\" % rmse)\n",
        "print(\"R-squared: %.2f\" % r2)\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = model.feature_importances_\n",
        "features = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6']  # Adjust these as per your dataset\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(features, feature_importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Actual vs Predicted\n",
        "for actual, predicted in zip(y_test, y_pred):\n",
        "    print(f\"Actual: {actual}, Predicted: {predicted:.2f}\")\n"
      ],
      "metadata": {
        "id": "IW8QgKZ6Ol0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MODEL 2 K-FOLD\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize measures\n",
        "scores_r2 = []\n",
        "scores_mse = []\n",
        "scores_rmse = []\n",
        "scores_mae = []\n",
        "feature_importances = []\n",
        "\n",
        "\n",
        "# K-Fold Cross-validation\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluation\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Storing results\n",
        "    scores_r2.append(r2)\n",
        "    scores_mse.append(mse)\n",
        "    scores_rmse.append(rmse)\n",
        "    scores_mae.append(mae)\n",
        "\n",
        "    feature_importances.append(model.feature_importances_)\n",
        "\n",
        "mean_feature_importances = mean(feature_importances, axis=0)\n",
        "\n",
        "# Print average scores\n",
        "print(\"Average R-squared: %.2f\" % np.mean(scores_r2))\n",
        "print(\"Average Mean Squared Error: %.2f\" % np.mean(scores_mse))\n",
        "print(\"Average Root Mean Squared Error: %.2f\" % np.mean(scores_rmse))\n",
        "print(\"Average Mean Absolute Error: %.2f\" % np.mean(scores_mae))\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for idx, imp in enumerate(mean_feature_importances):\n",
        "    print(f\"Feature {idx + 1}: {imp:.4f}\")"
      ],
      "metadata": {
        "id": "CGN7nBElUllZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bayesian Optimization\n",
        "\n",
        "param_space = {\n",
        "    'n_estimators': (10, 1000),  # Number of trees in the forest\n",
        "    'max_depth': (3, 50),  # Maximum depth of each tree\n",
        "    'min_samples_split': (2, 10),  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': (1, 10),  # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ('sqrt', 'log2')  # The number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    search_spaces=param_space,\n",
        "    n_iter=100,\n",
        "    cv=10,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bayes_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters found: {bayes_search.best_params_}\")"
      ],
      "metadata": {
        "id": "V-An2y6BOr0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing again based on these best params:\n",
        "\n",
        "# Best parameters found by BayesSearchCV\n",
        "best_params = bayes_search.best_params_\n",
        "\n",
        "# Initialize RandomForestRegressor with the best parameters\n",
        "\n",
        "optimized_model = RandomForestRegressor(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    min_samples_split=best_params['min_samples_split'],\n",
        "    min_samples_leaf=best_params['min_samples_leaf'],\n",
        "    max_features=best_params['max_features'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Setup K-Fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Calculate RMSE for each cross-validated fold using 'neg_root_mean_squared_error' which returns negative values\n",
        "rmse_scores = cross_val_score(optimized_model, X, y, scoring='neg_root_mean_squared_error', cv=kf, n_jobs=-1)\n",
        "\n",
        "# Convert scores to positive\n",
        "rmse_scores = -rmse_scores\n",
        "\n",
        "# Calculate average RMSE\n",
        "average_rmse = np.mean(rmse_scores)\n",
        "\n",
        "# Calculate R-squared for each fold\n",
        "r2_scores = cross_val_score(optimized_model, X, y, scoring='r2', cv=kf, n_jobs=-1)\n",
        "\n",
        "# Calculate average R-squared\n",
        "average_r2 = np.mean(r2_scores)\n",
        "\n",
        "# Calculate MAE for each fold using 'neg_mean_absolute_error' which returns negative values\n",
        "mae_scores = cross_val_score(optimized_model, X, y, scoring='neg_mean_absolute_error', cv=kf, n_jobs=-1)\n",
        "\n",
        "# Convert scores to positive\n",
        "mae_scores = -mae_scores\n",
        "\n",
        "# Calculate average MAE#\n",
        "average_mae = np.mean(mae_scores)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Average Root Mean Squared Error from K-Fold CV: {average_rmse:.2f}\")\n",
        "print(f\"Average R-squared from K-Fold CV: {average_r2:.2f}\")\n",
        "print(f\"Average Mean Absolute Error from K-Fold CV: {average_mae:.2f}\")"
      ],
      "metadata": {
        "id": "qZm1JJfRO0VX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}